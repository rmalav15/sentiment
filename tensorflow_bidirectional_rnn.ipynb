{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model is not complete, As the deadline was over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "#layers = tf.contrib.layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import FastText\n",
    "fasttxt_model = FastText.load_fasttext_format('/mnt/069A453E9A452B8D/Ram/Downloads/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastxt model similaritie of words:\n",
      "fraud, froud => 0.39351234\n",
      "awesome, awsm => 0.42913762\n",
      "awesome, good => 0.5001332\n"
     ]
    }
   ],
   "source": [
    "print(\"fastxt model similaritie of words:\")\n",
    "print(\"fraud, froud =>\", fasttxt_model.similarity('fraud', 'froud'))\n",
    "print(\"awesome, awsm =>\", fasttxt_model.similarity('awesome', 'awsm'))\n",
    "print(\"awesome, good =>\", fasttxt_model.similarity('awesome', 'good'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# preporcessed files with words replaced with voc index\n",
    "train_csv_path = \"train.csv\"\n",
    "val_csv_path = \"\"\n",
    "voc_file = \"\"\n",
    "summary_dir = \"\"\n",
    "\n",
    "vocab_size = 10000\n",
    "emb_size = 300\n",
    "\n",
    "\n",
    "NUM_BI_LAYER = 1\n",
    "NUM_HIDDEN = 100\n",
    "NUM_CLASSES = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "max_iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not complete\n",
    "train_data = tf.data.experimental.make_csv_dataset(train_csv_path, label_name=\"id\"\n",
    "                                                   batch_size=batch_size, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model and loss\n",
    "\n",
    "def BiRnn(text_batch, rating_batch, embedding_mat, reuse=False):\n",
    "    # Define the container of the parameter\n",
    "    Network = collections.namedtuple('Network', 'loss, pred_rating, \\\n",
    "                                                grads_and_vars, train, global_step, learning_rate')\n",
    "    with tf.variable_scope(\"birnn\", reuse=reuse):\n",
    "        # embedding lookup\n",
    "        embeddings = tf.nn.embedding_lookup(embedding_mat, text_batch)\n",
    "        \n",
    "        # Single bilstm layer\n",
    "        forward_lstm_cell = tf.nn.rnn_cell.LSTMCell(NUM_HIDDEN)\n",
    "        backward_lstm_cell = tf.nn.rnn_cell.LSTMCell(NUM_HIDDEN)\n",
    "        \n",
    "        (output_fw, output_bw), last_state = tf.nn.bidirectional_dynamic_rnn(forward_lstm_cell, \n",
    "                                                                             backward_lstm_cell, \n",
    "                                                                             embeddings)\n",
    "\n",
    "        bidir_output = tf.concat([output_fw, output_bw], axis=2)\n",
    "        \n",
    "        # Fully connected\n",
    "        logits = tf.contrib.layers.fully_connected(bidir_output, NUM_CLASSES, activation_fn=None)\n",
    "        \n",
    "        # prediction\n",
    "        pred_probs = tf.nn.softmax(logits)\n",
    "        \n",
    "    loss =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=rating_batch, logits=logits), \n",
    "                          name= \"loss\")\n",
    "    pred_rating = tf.argmax(pred_probs, 1)\n",
    "\n",
    "    with tf.variable_scope(\"global_step_and_learning_rate\", reuse=reuse):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps=10000,\n",
    "                                                   decay_rate = 0.1,\n",
    "                                                   staircase=True)\n",
    "        incr_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "    with tf.variable_scope(\"optimizer\", reuse=reuse):\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            tvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='birnn')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9)\n",
    "            grads_and_vars = optimizer.compute_gradients(total_loss, tvars)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    return Network(\n",
    "        loss = loss,\n",
    "        pred_rating = pred_rating,\n",
    "        grads_and_vars=grads_and_vars,\n",
    "        train=tf.group(total_loss, incr_global_step, train_op),\n",
    "        global_step=global_step,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding matrix (Should be intialized with fasttext)\n",
    "embedding_mat = np.random.rand(vocab_size,emb_size)\n",
    "\n",
    "# Dummy input \n",
    "dummy_input_indices = np.random.randint(vocab_size, size=(batch_size, 10))\n",
    "dummy_labels = np.random.randint(1, high=6, size=(batch_size,))\n",
    "\n",
    "# Slightly bad implemenataion,\n",
    "# need to take care of two scoper(\"birnn, embeddings\") while saving and loading model\n",
    "# For now keeping it non-trainable\n",
    "with tf.variable_scope(\"embeddings\"):\n",
    "    embadding_mat = tf.Variable(tf.constant(0.0, shape=[vocab_size, emb_size]),\n",
    "                    trainable=False, name=\"W\")\n",
    "\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, emb_size])\n",
    "    embedding_init = embadding_mat.assign(embedding_placeholder)\n",
    "\n",
    "# input placeholder\n",
    "text_placeholder = tf.placeholder(tf.int32, shape=[batch_size, None], name=\"input_text\")\n",
    "rating_placeholder = tf.placeholder(tf.int8, shape=[batch_size,], name=\"input_labels\")\n",
    "\n",
    "\n",
    "# building network\n",
    "net = BiRnn(text_placeholder, rating_placeholder, embadding_mat)\n",
    "print('Finish building the birnn !!!')\n",
    "\n",
    "# Start the session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Use supervisor to coordinate all queue and summary writer\n",
    "# Deprecated, Update with tf.train.MonitoredTrainingSession\n",
    "sv = tf.train.Supervisor(logdir=summary_dir, save_summaries_secs=0, saver=None)\n",
    "\n",
    "# Training\n",
    "with sv.managed_session(config=config) as sess:\n",
    "    for step in range(max_iter):\n",
    "            fetches = {\n",
    "                \"train\": net.train,\n",
    "                \"global_step\": sv.global_step,\n",
    "                \"loss\": net.loss,\n",
    "                \"learning_rate\": net.learning_rate\n",
    "            }\n",
    "            \n",
    "            result = sess.run(fetches, feed_dict={embedding_placeholder: embedding_mat,\n",
    "                                                 text_placeholder:dummy_input_indices,\n",
    "                                                 rating_placeholder:dummy_labels})\n",
    "            print(\"global_step: \", results[\"global_step\"])\n",
    "            print(\"learning_rate: \", results['learning_rate'])\n",
    "            print(\"current_loss: \", results['loss'])\n",
    "            print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
